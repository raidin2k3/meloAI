{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZFYWiMr-rvOV",
    "outputId": "417e53b3-d521-48ce-e5af-efea4cf6633e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\lab\\anaconda3\\lib\\site-packages (2.3.1+cu121)\n",
      "Requirement already satisfied: transformers in c:\\users\\lab\\anaconda3\\lib\\site-packages (4.46.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\lab\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\lab\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\lab\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\lab\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lab\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\lab\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\lab\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\lab\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\lab\\anaconda3\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\lab\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\lab\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\lab\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orchvision (c:\\Users\\Lab\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orchvision (c:\\Users\\Lab\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orchvision (c:\\Users\\Lab\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Oi-TbCcRrvOW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2LMHeadModel, GPT2Tokenizer, GemmaTokenizer, BitsAndBytesConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from huggingface_hub import login\n",
    "import sentencepiece\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from accelerate import infer_auto_device_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model v1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Wikitext dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert custom lines into a dataset\n",
    "custom_dataset = Dataset.from_dict({\"text\": custom_lines})\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# Assign a padding token if not already present\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as the pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87136afa4c7e4fb785055ff6ed7741b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93398fe4cc534d668bd7be9bf15750e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98966cbff520458484e77dd275efb616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b845412545484a48841172afc083af5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"text\"], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=512\n",
    "    )\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()  # Set the labels as input_ids\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Tokenize Wikitext dataset\n",
    "tokenized_wikitext = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Tokenize custom dataset\n",
    "custom_data = [{\"text\": line} for line in custom_lines]  # Convert your custom lines into a dictionary list\n",
    "custom_dataset = Dataset.from_list(custom_data)  # Create a dataset from custom lines\n",
    "tokenized_custom = custom_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Combine the datasets using concatenate_datasets\n",
    "combined_train_dataset = concatenate_datasets([tokenized_wikitext[\"train\"], tokenized_custom])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "combined_train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "def collate_fn(batch):\n",
    "    return tokenizer.pad(batch, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    combined_train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=1,  # Disable multiprocessing to debug\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2296 [00:00<?, ?it/s]c:\\Users\\Lab\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:545: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|██████████| 2296/2296 [17:45<00:00,  2.15it/s]\n",
      "100%|██████████| 2296/2296 [17:43<00:00,  2.16it/s]\n",
      "100%|██████████| 2296/2296 [20:08<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "scaler = GradScaler()  # Initialize the scaler for mixed precision\n",
    "\n",
    "# Set gradient accumulation steps (adjust to simulate larger batches)\n",
    "accumulation_steps = 16  # Simulates larger batch size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(train_dataloader, leave=True)\n",
    "\n",
    "    optimizer.zero_grad()  # Reset the gradients before starting\n",
    "\n",
    "    for step, batch in enumerate(loop):\n",
    "        inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "\n",
    "        with torch.cuda.amp.autocast():  # Enable mixed precision\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss / accumulation_steps  # Scale loss for accumulation\n",
    "\n",
    "        scaler.scale(loss).backward()  # Backpropagate loss\n",
    "\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)  # Update weights\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jUqdlTMrvOW",
    "outputId": "b5299e07-2112-4bc2-c99b-c7d2e788f517"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lab\\AppData\\Local\\Temp\\ipykernel_3024\\3147443359.py:51: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Initialize the scaler for mixed precision\n",
      "  0%|          | 0/2295 [00:00<?, ?it/s]C:\\Users\\Lab\\AppData\\Local\\Temp\\ipykernel_3024\\3147443359.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Enable mixed precision\n",
      "c:\\Users\\Lab\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:545: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|██████████| 2295/2295 [47:35<00:00,  1.24s/it]\n",
      "100%|██████████| 2295/2295 [42:19<00:00,  1.11s/it]\n",
      "100%|██████████| 2295/2295 [42:27<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#OLD\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# Assign a padding token if not already present\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as the pad_token\n",
    "\n",
    "# Tokenize the input text and set up labels\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()  # Set the labels as input_ids\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Convert dataset to PyTorch tensors\n",
    "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tokenizer.pad(batch, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=1,  # Disable multiprocessing to debug\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "scaler = GradScaler()  # Initialize the scaler for mixed precision\n",
    "\n",
    "# Set gradient accumulation steps (adjust to simulate larger batches)\n",
    "accumulation_steps = 4  # Simulates larger batch size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(train_dataloader, leave=True)\n",
    "\n",
    "    optimizer.zero_grad()  # Reset the gradients before starting\n",
    "\n",
    "    for step, batch in enumerate(loop):\n",
    "        inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "\n",
    "        with torch.cuda.amp.autocast():  # Enable mixed precision\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss / accumulation_steps  # Scale loss for accumulation\n",
    "\n",
    "        scaler.scale(loss).backward()  # Backpropagate loss\n",
    "\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)  # Update weights\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ndYFoHEervOX",
    "outputId": "af782b9e-2b08-472c-c1b1-57dc0b2f3f45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stnV_zhPrvOX",
    "outputId": "9049ff36-a8cb-45c2-faac-05c6250faed5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me about tesla.\n",
      "I'm not sure if you know what I mean, but it's a very good name for the game and one that is really fun to play with friends or just have some time together in your own home (or maybe even on vacation). It has an interesting twist where each character can be played as either of two different characters who are fighting against their respective enemies at once! The story starts off pretty simple: You're playing this guy named TESLA from \"The Legend of Zelda\" series which was released back when Nintendo launched its first console called NX . He had been working out his new job after he got sick so decided to go into hiding because there were no jobs available anymore due all sorts people wanted\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set pad token to eos token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Move model to the correct device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Prepare input text and move to device\n",
    "input_text = \"Tell me about tesla\"\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Create attention mask to differentiate between padding and actual data\n",
    "attention_mask = torch.ones(inputs.shape, device=device)\n",
    "\n",
    "# Generate text with repetition penalty\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=150,  # Increased length to avoid truncation\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        temperature=0.8,  # Slightly adjusted temperature\n",
    "        top_k=50,         # Limit next token choices to top-k\n",
    "        top_p=0.9,        # Use nucleus sampling\n",
    "        repetition_penalty=1.2  # Penalty to discourage repetition\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HoQLOtj5rvOY"
   },
   "outputs": [],
   "source": [
    "# Define your custom dataset (replace with your own sentences)\n",
    "custom_lines = [\n",
    "    \"Subject: Proposal for a Strategic Partnership Opportunity - Dear [Recipient's Name], I hope this email finds you well. My name is [Your Name], and I am [Your Position] at [Your Company]. I am reaching out to explore a potential partnership where we can combine our strengths for mutual growth. Let me know a suitable time to discuss further. Warm regards, [Your Name].\",\n",
    "    \"Subject: Let's Collaborate for a Mutual Marketing Success - Dear [Recipient's Name], I am [Your Name] from [Your Company], and I would like to propose a co-marketing initiative to enhance visibility for both our brands. Our combined efforts can deliver great value to our audience. Looking forward to your response. Best regards, [Your Name].\",\n",
    "    \"Subject: Proposal for Collaborative Product Development - Dear [Recipient's Name], At [Your Company], we admire your work in [specific area]. We see an opportunity to collaborate on developing a product that blends our expertise for market success. Please let me know a convenient time to discuss this further. Kind regards, [Your Name].\",\n",
    "    \"Subject: Partnering for a Greener Future - Dear [Recipient's Name], Sustainability is integral to [Your Company], and we see [Recipient's Company] as a perfect partner to amplify our efforts. Together, we can tackle [specific issue]. I would love to discuss our shared goals. Warm regards, [Your Name].\",\n",
    "    \"Subject: Let's Innovate Together - Dear [Recipient's Name], Your advancements in [specific technology] inspire us, and we believe a partnership between [Your Company] and [Recipient's Company] could lead to groundbreaking innovations. Let’s connect to discuss this exciting opportunity. Best regards, [Your Name].\",\n",
    "    \"Subject: Exploring a Cross-Promotion Partnership - Dear [Recipient's Name], I am reaching out to propose a cross-promotion opportunity between [Your Company] and [Recipient's Company]. By collaborating, we can engage broader audiences and drive mutual growth. Let’s discuss the possibilities. Kind regards, [Your Name].\",\n",
    "    \"Subject: Partnership Opportunity for Regional Expansion - Dear [Recipient's Name], As [Your Company] plans to expand in [specific region], we believe [Recipient's Company] would be an ideal partner given your strong presence in the area. Let’s discuss this exciting opportunity. Best regards, [Your Name].\",\n",
    "    \"Subject: Let’s Host a Joint Event! - Dear [Recipient's Name], I am [Your Name], [Your Position] at [Your Company]. I propose a partnership to host a joint event or webinar showcasing our expertise in [specific topic]. Together, we can create high-value content. Looking forward to your thoughts. Warm regards, [Your Name].\",\n",
    "    \"Subject: Partnering to Optimize Supply Chain - Dear [Recipient's Name], I am writing to discuss a potential partnership with [Recipient's Company] to streamline and optimize our supply chain operations for mutual benefits. Could we meet to explore this? Kind regards, [Your Name].\",\n",
    "    \"Subject: Join Us in Shaping the Future of [Industry/Field] - Dear [Recipient's Name], [Your Company] is embarking on an R&D initiative in [specific area] and would love to collaborate with [Recipient's Company] to achieve groundbreaking advancements. Please let me know your availability. Best regards, [Your Name].\"\n",
    "]\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "custom_data = [{\"text\": line} for line in custom_lines]\n",
    "custom_dataset = Dataset.from_list(custom_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to C:\\Users\\Lab\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ffd1f79bb642ff82525857874e3d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer for GeMMA-2B\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the examples\n",
    "    tokenized_inputs = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    \n",
    "    # Convert input_ids to tensor (if not already) and clone for labels\n",
    "    tokenized_inputs[\"input_ids\"] = torch.tensor(tokenized_inputs[\"input_ids\"])\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].clone()  # Ensure labels are set correctly\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_custom_dataset = custom_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_custom_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd7c1d98d224d81a155a5eebbad7ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the GeMMA-2B model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\")\n",
    "\n",
    "# Move model to GPU/TPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_custom_dataset,\n",
    "    batch_size=2,  # Adjust based on GPU memory\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Mixed precision setup\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]c:\\Users\\Lab\\anaconda3\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:376: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Epoch 1: 100%|██████████| 5/5 [02:46<00:00, 33.20s/it, loss=11.7]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Initialize necessary components\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Ensure model is on the correct device\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)  # Or any other optimizer you're using\n",
    "scaler = GradScaler()  # For mixed precision training\n",
    "\n",
    "# Assuming train_dataloader is already set up (e.g., using DataLoader from torch.utils.data)\n",
    "\n",
    "epochs = 3  # Adjust as needed\n",
    "model.train()  # Set the model in training mode\n",
    "\n",
    "accumulation_steps = 16  # Try increasing the number of accumulation steps\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(train_dataloader, leave=True)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for step, batch in enumerate(loop):\n",
    "        inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "\n",
    "        with torch.cuda.amp.autocast():  # Use mixed precision\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs['loss'] if 'loss' in outputs else compute_loss(outputs, inputs)\n",
    "\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Perform optimizer step after accumulation_steps\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        loop.set_description(f\"Epoch {epoch + 1}\")\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fine_tuned_gemma_2b\\\\tokenizer_config.json',\n",
       " 'fine_tuned_gemma_2b\\\\special_tokens_map.json',\n",
       " 'fine_tuned_gemma_2b\\\\tokenizer.model',\n",
       " 'fine_tuned_gemma_2b\\\\added_tokens.json',\n",
       " 'fine_tuned_gemma_2b\\\\tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"fine_tuned_gemma_2b\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_gemma_2b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273630f54f234ab0a787ffd5f506fcf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify your model's directory or name\n",
    "model_name = \"fine_tuned_gemma_2b\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_tex(userinput):\n",
    "    # Tokenize the input text with truncation, padding, and max_length handling\n",
    "    tokenized_input = tokenizer(\n",
    "        userinput,\n",
    "        return_tensors=\"pt\",  # Output PyTorch tensors\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    # Move input_ids and attention_mask to the same device as the model\n",
    "    input_ids = tokenized_input['input_ids'].to(device)\n",
    "    attention_mask = tokenized_input['attention_mask'].to(device)\n",
    "\n",
    "    # Generate text using the model\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,               # Provide input_ids\n",
    "        attention_mask=attention_mask,     # Optional but useful for padded sequences\n",
    "        max_length=1000,                    # Adjust max_length for the desired output\n",
    "        num_return_sequences=1,            # Generate one sequence\n",
    "        no_repeat_ngram_size=2,            # Avoid repetition\n",
    "        temperature=0.7,                   # Add randomness\n",
    "        top_k=50,                          # Top-k sampling\n",
    "        top_p=0.95,                        # Nucleus sampling\n",
    "        do_sample=True,                    # Enable sampling\n",
    "        early_stopping=True                # Stop if confident\n",
    "    )\n",
    "\n",
    "    # Decode the generated tokens into text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give an example of a mail to Tesla for partnership.\n",
      "\n",
      "The data in the table below are the amounts of crude oil imported into the United States from the Organization of Petroleum Exporting Countries (OPEC) for the years 1995–2015. (The years are given in order, with $t=5$ corresponding to 2896.)\n",
      "\n",
      "<strong>a.</strong> Plot the data, letting $ t $ be the year going from $1$ to $30$ starting with the $ 5 $ in $2985 $. Use the window $ x $ from 0 to above $50 $ and $ y $from $00 3 $ above zero to about $40 $. <strong>b. </strong>Find a quartic function that models these data. Plot both the function and the points on the same axes. How well does the quaric model from part a fit the given data? \n",
      "\n",
      "$x$ is the number of years after $905$, $y$is the amount of import $(in millions of barrels per day), where $x = 4$ corresponds to the start of 910.\n"
     ]
    }
   ],
   "source": [
    "# User input handling\n",
    "userinput = input(\"Enter your text: \").strip()\n",
    "\n",
    "# Generate and print the output\n",
    "print(gen_tex(userinput))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model v3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to C:\\Users\\Lab\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"princeton-nlp/Sheared-LLaMA-1.3B\")\n",
    "model2 = AutoModelForCausalLM.from_pretrained(\"princeton-nlp/Sheared-LLaMA-1.3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your custom dataset (replace with your own sentences)\n",
    "data = [\n",
    "    \"Subject: Proposal for a Strategic Partnership Opportunity - Dear [Recipient's Name], I hope this email finds you well. My name is [Your Name], and I am [Your Position] at [Your Company]. I am reaching out to explore a potential partnership where we can combine our strengths for mutual growth. Let me know a suitable time to discuss further. Warm regards, [Your Name].\",\n",
    "    \"Subject: Let's Collaborate for a Mutual Marketing Success - Dear [Recipient's Name], I am [Your Name] from [Your Company], and I would like to propose a co-marketing initiative to enhance visibility for both our brands. Our combined efforts can deliver great value to our audience. Looking forward to your response. Best regards, [Your Name].\",\n",
    "    \"Subject: Proposal for Collaborative Product Development - Dear [Recipient's Name], At [Your Company], we admire your work in [specific area]. We see an opportunity to collaborate on developing a product that blends our expertise for market success. Please let me know a convenient time to discuss this further. Kind regards, [Your Name].\",\n",
    "    \"Subject: Partnering for a Greener Future - Dear [Recipient's Name], Sustainability is integral to [Your Company], and we see [Recipient's Company] as a perfect partner to amplify our efforts. Together, we can tackle [specific issue]. I would love to discuss our shared goals. Warm regards, [Your Name].\",\n",
    "    \"Subject: Let's Innovate Together - Dear [Recipient's Name], Your advancements in [specific technology] inspire us, and we believe a partnership between [Your Company] and [Recipient's Company] could lead to groundbreaking innovations. Let’s connect to discuss this exciting opportunity. Best regards, [Your Name].\",\n",
    "    \"Subject: Exploring a Cross-Promotion Partnership - Dear [Recipient's Name], I am reaching out to propose a cross-promotion opportunity between [Your Company] and [Recipient's Company]. By collaborating, we can engage broader audiences and drive mutual growth. Let’s discuss the possibilities. Kind regards, [Your Name].\",\n",
    "    \"Subject: Partnership Opportunity for Regional Expansion - Dear [Recipient's Name], As [Your Company] plans to expand in [specific region], we believe [Recipient's Company] would be an ideal partner given your strong presence in the area. Let’s discuss this exciting opportunity. Best regards, [Your Name].\",\n",
    "    \"Subject: Let’s Host a Joint Event! - Dear [Recipient's Name], I am [Your Name], [Your Position] at [Your Company]. I propose a partnership to host a joint event or webinar showcasing our expertise in [specific topic]. Together, we can create high-value content. Looking forward to your thoughts. Warm regards, [Your Name].\",\n",
    "    \"Subject: Partnering to Optimize Supply Chain - Dear [Recipient's Name], I am writing to discuss a potential partnership with [Recipient's Company] to streamline and optimize our supply chain operations for mutual benefits. Could we meet to explore this? Kind regards, [Your Name].\",\n",
    "    \"Subject: Join Us in Shaping the Future of [Industry/Field] - Dear [Recipient's Name], [Your Company] is embarking on an R&D initiative in [specific area] and would love to collaborate with [Recipient's Company] to achieve groundbreaking advancements. Please let me know your availability. Best regards, [Your Name].\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# Assign eos_token as pad_token if pad_token is not defined\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Now tokenize the data with padding\n",
    "inputs = tokenizer(data, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "dataset2 = CustomDataset(inputs)\n",
    "dataloader2 = DataLoader(dataset2, batch_size=1, shuffle=True)\n",
    "optimizer2 = AdamW(model2.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lab\\AppData\\Local\\Temp\\ipykernel_20192\\447680108.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "c:\\Users\\Lab\\anaconda3\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:602: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Epoch 0: 100%|██████████| 10/10 [00:15<00:00,  1.56s/it, loss=0.362]\n",
      "Epoch 1: 100%|██████████| 10/10 [00:20<00:00,  2.00s/it, loss=0.105]\n",
      "Epoch 2: 100%|██████████| 10/10 [00:20<00:00,  2.00s/it, loss=0.0405]\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model2.to(device)\n",
    "scaler = GradScaler()\n",
    "model2.train()\n",
    "model2.gradient_checkpointing_enable()  # Enable gradient checkpointing\n",
    "\n",
    "accumulation_steps = 8  # Gradient accumulation\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(dataloader2, leave=True)\n",
    "    optimizer2.zero_grad()\n",
    "    for step, batch in enumerate(loop):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with autocast():  # Mixed precision training\n",
    "            outputs = model2(**batch, labels=batch[\"input_ids\"])\n",
    "            loss2 = outputs.loss\n",
    "            loss2 = loss2 / accumulation_steps  # Normalize loss for gradient accumulation\n",
    "            scaler.scale(loss2).backward()\n",
    "\n",
    "        if (step + 1) % accumulation_steps == 0 or step == len(dataloader2) - 1:\n",
    "            scaler.step(optimizer2)\n",
    "            scaler.update()\n",
    "            optimizer2.zero_grad()\n",
    "\n",
    "        loop.set_description(f\"Epoch {epoch}\")\n",
    "        loop.set_postfix(loss=loss2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine-tuned-1.3b\\\\tokenizer_config.json',\n",
       " './fine-tuned-1.3b\\\\special_tokens_map.json',\n",
       " './fine-tuned-1.3b\\\\tokenizer.model',\n",
       " './fine-tuned-1.3b\\\\added_tokens.json',\n",
       " './fine-tuned-1.3b\\\\tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.save_pretrained(\"./fine-tuned-1.3b\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-1.3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62b3406408f420da52c6f725164c25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify your model's directory or name\n",
    "model_name = \"fine-tuned-1.3b\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_tex(userinput):\n",
    "    # Tokenize the input text with truncation, padding, and max_length handling\n",
    "    tokenized_input = tokenizer(\n",
    "        userinput,\n",
    "        return_tensors=\"pt\",  # Output PyTorch tensors\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512  # Limit to a reasonable max_length to prevent overgeneration\n",
    "    )\n",
    "\n",
    "    # Move input_ids and attention_mask to the same device as the model\n",
    "    input_ids = tokenized_input['input_ids'].to(device)\n",
    "    attention_mask = tokenized_input['attention_mask'].to(device)\n",
    "\n",
    "    # Generate text using the model\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,               # Provide input_ids\n",
    "        attention_mask=attention_mask,     # Optional but useful for padded sequences\n",
    "        max_length=500,                    # Increase max_length for a longer output\n",
    "        num_return_sequences=1,            # Generate one sequence\n",
    "        no_repeat_ngram_size=2,            # Avoid repetition of phrases\n",
    "        temperature=0.7,                   # Control randomness in the output\n",
    "        top_k=50,                          # Top-k sampling for diverse output\n",
    "        top_p=0.95,                        # Nucleus sampling to keep quality\n",
    "        do_sample=True,                    # Enable sampling for more variety\n",
    "        early_stopping=True                # Stop early when confident\n",
    "    )\n",
    "\n",
    "    # Decode the generated tokens into text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Prevent the model from repeating the user input at the start of the generated text\n",
    "    if generated_text.lower().startswith(userinput.lower()):\n",
    "        # Strip the user input from the result to avoid repetition\n",
    "        generated_text = generated_text[len(userinput):].strip()\n",
    "\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "# User input handling\n",
    "userinput = input(\"Enter your text: \").strip()\n",
    "\n",
    "# Generate and print the output\n",
    "print(gen_tex(userinput))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
