{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFYWiMr-rvOV",
        "outputId": "417e53b3-d521-48ce-e5af-efea4cf6633e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in c:\\users\\lab\\anaconda3\\lib\\site-packages (2.4.1+cu124)\n",
            "Requirement already satisfied: transformers in c:\\users\\lab\\anaconda3\\lib\\site-packages (4.45.2)\n",
            "Requirement already satisfied: datasets in c:\\users\\lab\\anaconda3\\lib\\site-packages (3.0.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\lab\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\lab\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\lab\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\lab\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: setuptools in c:\\users\\lab\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: requests in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\lab\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in c:\\users\\lab\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\lab\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\lab\\anaconda3\\lib\\site-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: colorama in c:\\users\\lab\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install torch transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oi-TbCcRrvOW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jUqdlTMrvOW",
        "outputId": "b5299e07-2112-4bc2-c99b-c7d2e788f517"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Lab\\AppData\\Local\\Temp\\ipykernel_3024\\3147443359.py:51: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()  # Initialize the scaler for mixed precision\n",
            "  0%|          | 0/2295 [00:00<?, ?it/s]C:\\Users\\Lab\\AppData\\Local\\Temp\\ipykernel_3024\\3147443359.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():  # Enable mixed precision\n",
            "c:\\Users\\Lab\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:545: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
            "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
            "100%|██████████| 2295/2295 [47:35<00:00,  1.24s/it]\n",
            "100%|██████████| 2295/2295 [42:19<00:00,  1.11s/it]\n",
            "100%|██████████| 2295/2295 [42:27<00:00,  1.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training complete!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
        "\n",
        "# Assign a padding token if not already present\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as the pad_token\n",
        "\n",
        "# Tokenize the input text and set up labels\n",
        "def tokenize_function(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()  # Set the labels as input_ids\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Convert dataset to PyTorch tensors\n",
        "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = tokenized_datasets[\"train\"]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tokenizer.pad(batch, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=1,  # Disable multiprocessing to debug\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Load the model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Set up the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Set the model to training mode\n",
        "model.train()\n",
        "\n",
        "# Training loop\n",
        "epochs = 3\n",
        "scaler = GradScaler()  # Initialize the scaler for mixed precision\n",
        "\n",
        "# Set gradient accumulation steps (adjust to simulate larger batches)\n",
        "accumulation_steps = 4  # Simulates larger batch size\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    loop = tqdm(train_dataloader, leave=True)\n",
        "\n",
        "    optimizer.zero_grad()  # Reset the gradients before starting\n",
        "\n",
        "    for step, batch in enumerate(loop):\n",
        "        inputs = {key: val.to(device) for key, val in batch.items()}\n",
        "\n",
        "        with torch.cuda.amp.autocast():  # Enable mixed precision\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs.loss / accumulation_steps  # Scale loss for accumulation\n",
        "\n",
        "        scaler.scale(loss).backward()  # Backpropagate loss\n",
        "\n",
        "        if (step + 1) % accumulation_steps == 0:\n",
        "            scaler.step(optimizer)  # Update weights\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndYFoHEervOX",
        "outputId": "af782b9e-2b08-472c-c1b1-57dc0b2f3f45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stnV_zhPrvOX",
        "outputId": "9049ff36-a8cb-45c2-faac-05c6250faed5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Lab\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Lab\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How are you? Tell me about Tesla Motors!\n",
            "I'm a big fan of the company and I've been working on it for years. It's one of my favorite cars ever made, but there is something special that makes this car so unique to us: The way we build our vehicles in such an innovative manner allows them not only to be built with high quality materials (like aluminum), they can also have their own custom parts available from suppliers like BMW or Mercedes-Benz as well – all without having to worry too much over what will happen when these components go into production…and then get shipped out by truck every year.\"\n"
          ]
        }
      ],
      "source": [
        "# Load the model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Set pad token to eos token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Move model to the correct device (GPU or CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Prepare input text and move to device\n",
        "input_text = \"How are you? Tell me about Tesla Motors!\"\n",
        "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Create attention mask to differentiate between padding and actual data\n",
        "attention_mask = torch.ones(inputs.shape, device=device)\n",
        "\n",
        "# Generate text with repetition penalty\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=150,  # Increased length to avoid truncation\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        temperature=0.8,  # Slightly adjusted temperature\n",
        "        top_k=50,         # Limit next token choices to top-k\n",
        "        top_p=0.9,        # Use nucleus sampling\n",
        "        repetition_penalty=1.2  # Penalty to discourage repetition\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoQLOtj5rvOY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}