{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/cu121/torch_stable.html\n",
      "Requirement already satisfied: praw in c:\\users\\lab\\anaconda3\\lib\\site-packages (7.8.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\lab\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: vaderSentiment in c:\\users\\lab\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\lab\\anaconda3\\lib\\site-packages (2.153.0)\n",
      "Requirement already satisfied: flask in c:\\users\\lab\\anaconda3\\lib\\site-packages (3.0.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\lab\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\lab\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\lab\\anaconda3\\lib\\site-packages (2.17.1)\n",
      "Requirement already satisfied: tf-keras in c:\\users\\lab\\anaconda3\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: transformers==4.46.2 in c:\\users\\lab\\anaconda3\\lib\\site-packages (4.46.2)\n",
      "Requirement already satisfied: torch==2.3.1+cu121 in c:\\users\\lab\\anaconda3\\lib\\site-packages (2.3.1+cu121)\n",
      "Requirement already satisfied: torchvision==0.18.1+cu121 in c:\\users\\lab\\anaconda3\\lib\\site-packages (0.18.1+cu121)\n",
      "Requirement already satisfied: accelerate in c:\\users\\lab\\anaconda3\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: google-generativeai in c:\\users\\lab\\anaconda3\\lib\\site-packages (0.8.3)\n",
      "Requirement already satisfied: pyngrok in c:\\users\\lab\\anaconda3\\lib\\site-packages (7.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers==4.46.2) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers==4.46.2) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers==4.46.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers==4.46.2) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers==4.46.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers==4.46.2) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers==4.46.2) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers==4.46.2) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers==4.46.2) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from transformers==4.46.2) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from torch==2.3.1+cu121) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\lab\\anaconda3\\lib\\site-packages (from torch==2.3.1+cu121) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\lab\\anaconda3\\lib\\site-packages (from torch==2.3.1+cu121) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from torch==2.3.1+cu121) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lab\\anaconda3\\lib\\site-packages (from torch==2.3.1+cu121) (2024.3.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from torch==2.3.1+cu121) (2021.4.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from torchvision==0.18.1+cu121) (10.3.0)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from google-api-python-client) (2.36.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from google-api-python-client) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from google-api-python-client) (2.23.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from flask) (3.0.3)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from flask) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from flask) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from flask) (1.6.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\lab\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\lab\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\lab\\anaconda3\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.1 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from tensorflow) (2.17.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow) (4.25.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lab\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow) (1.68.0)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\lab\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from google-generativeai) (0.6.10)\n",
      "Requirement already satisfied: pydantic in c:\\users\\lab\\anaconda3\\lib\\site-packages (from google-generativeai) (2.5.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from google-ai-generativelanguage==0.6.10->google-generativeai) (1.25.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lab\\anaconda3\\lib\\site-packages (from click>=8.1.3->flask) (0.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.66.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (4.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from jinja2->torch==2.3.1+cu121) (2.1.3)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\lab\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.1+cu121) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\lab\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.1+cu121) (2021.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from requests->transformers==4.46.2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from requests->transformers==4.46.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from requests->transformers==4.46.2) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from requests->transformers==4.46.2) (2024.8.30)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (2.14.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from sympy->torch==2.3.1+cu121) (1.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.1->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.62.3)\n",
      "Requirement already satisfied: rich in c:\\users\\lab\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.1->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\lab\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.1->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\lab\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.1->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.8)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.1->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.1->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.1->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lab\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.1->tensorflow) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orchvision (c:\\Users\\Lab\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orchvision (c:\\Users\\Lab\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orchvision (c:\\Users\\Lab\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "#installing libs\n",
    "%pip install praw pandas vaderSentiment google-api-python-client flask datasets scikit-learn tensorflow tf-keras transformers==4.46.2 torch==2.3.1+cu121 torchvision==0.18.1+cu121 -f https://download.pytorch.org/whl/cu121/torch_stable.html accelerate google-generativeai pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Lab\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#importing required libs\n",
    "import pandas as pd\n",
    "import praw\n",
    "from praw.models import MoreComments\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "import googleapiclient.discovery\n",
    "from googleapiclient.errors import HttpError\n",
    "from werkzeug.serving import run_simple\n",
    "import threading\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EvalPrediction, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from flask import Flask, render_template_string, request, session\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import google.generativeai as genai\n",
    "from pyngrok import ngrok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAGE 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping Reddit for Dataset to Train on Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.7.1 of praw is outdated. Version 7.8.1 was released Friday October 25, 2024.\n"
     ]
    }
   ],
   "source": [
    "#reddit creds\n",
    "reddit = praw.Reddit(\n",
    "    user_agent=\"Comment Extraction (by /u/lestergreeks)\",\n",
    "    client_id=\"YiK4KHJXxneFv0IiV8aOhg\", \n",
    "    client_secret=\"8GfGLUx8E62B3sBUwJcie43RDBQm7A\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting all comments from top10 posts of a subreddit\n",
    "subreddit = reddit.subreddit('technews')\n",
    "\n",
    "all_comments = pd.DataFrame()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_comments(submission):\n",
    "    posts = []\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    count = 0\n",
    "    for top_level_comment in submission.comments.list():\n",
    "        if isinstance(top_level_comment, MoreComments):\n",
    "            continue\n",
    "        if (top_level_comment.author and \"bot\" not in top_level_comment.author.name.lower() \n",
    "            and not top_level_comment.stickied):\n",
    "            posts.append(top_level_comment.body.encode('utf-8', 'ignore').decode('utf-8'))\n",
    "    \n",
    "    return posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_posts():\n",
    "    global all_comments\n",
    "    attempt = 0\n",
    "    for submission in subreddit.rising(limit=15):\n",
    "        try:\n",
    "            print(f\"Processing post: {submission.title}\")\n",
    "            posts = extract_comments(submission)\n",
    "            posts_df = pd.DataFrame(posts, columns=[\"body\"])\n",
    "            indexNames = posts_df[(posts_df.body == '[removed]') | (posts_df.body == '[deleted]')].index\n",
    "            posts_df.drop(indexNames, inplace=True)\n",
    "            posts_df['post_title'] = submission.title\n",
    "            posts_df['post_time'] = datetime.fromtimestamp(submission.created_utc, tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            all_comments = pd.concat([all_comments, posts_df], ignore_index=True)\n",
    "        \n",
    "        except praw.exceptions.RedditAPIException as e:\n",
    "            if \"RATELIMIT\" in str(e):\n",
    "                attempt += 1\n",
    "                sleep_time = 120 * attempt\n",
    "                print(f\"Rate limit hit. Sleeping for {sleep_time} seconds. Error: {e}\")\n",
    "                time.sleep(sleep_time) \n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Encountered an error: {e}\")\n",
    "                continue\n",
    "    \n",
    "        time.sleep(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing post: Cybertruck's Many Recalls Make It Worse Than 91 Percent of All 2024 Vehicles\n",
      "Processing post: Researchers jailbreak AI robots to run over pedestrians, place bombs for maximum damage, and covertly spy\n",
      "Processing post: Valve first came up with the Steam Hardware Survey more than 20 years ago because it wanted to know what specs it should target for Half-Life 2\n",
      "Processing post: Threads' latest test will finally let you make the ‘following’ feed the default\n",
      "Processing post: Bluesky breaching rules around disclosure of information, says EU\n",
      "Processing post: The Future of Online Privacy Hinges on Thousands of New Jersey Cops\n",
      "Processing post: Most Gen Zers are terrified of AI taking their jobs. Their bosses consider themselves immune\n",
      "Processing post: Sony’s making a handheld console to compete with Nintendo and Microsoft | The portable console could natively play PS5 games without an active Wi-Fi connection.\n",
      "Processing post: Anthropic proposes a new way to connect data to AI chatbots | TechCrunch\n",
      "Processing post: Tens of millions of devices are thrown away each year — and the rise of generative AI will only make this worse\n",
      "Processing post: AI increasingly used for sextortion, scams and child abuse, says senior UK police chief\n",
      "Processing post: Ubitium announces development of 'universal' processor that combines CPU, GPU, DSP, and FPGA functionalities – RISC-V powered chip slated to arrive in two years\n",
      "Processing post: Elizabeth Warren calls for crackdown on Internet “monopoly” you’ve never heard of. Senator wants to investigate whether VeriSign is ripping off customers and violating antitrust laws.\n",
      "Processing post: Droidspeak: AI models work together faster when they speak their own language\n",
      "Processing post: Dangerous global botnet fueling residential proxies is being hit in major crackdown | Lumen and partners disrupt operations of NSOCKS proxy\n"
     ]
    }
   ],
   "source": [
    "process_posts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>post_title</th>\n",
       "      <th>post_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I wonder which cars are in the remaining 9%?</td>\n",
       "      <td>Cybertruck's Many Recalls Make It Worse Than 9...</td>\n",
       "      <td>2024-11-25 13:06:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Since launch, Tesla's polarizing electric pick...</td>\n",
       "      <td>Cybertruck's Many Recalls Make It Worse Than 9...</td>\n",
       "      <td>2024-11-25 13:06:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>which 9% of cars make up a list of vehicles WO...</td>\n",
       "      <td>Cybertruck's Many Recalls Make It Worse Than 9...</td>\n",
       "      <td>2024-11-25 13:06:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It was never purchased for utility, so to the ...</td>\n",
       "      <td>Cybertruck's Many Recalls Make It Worse Than 9...</td>\n",
       "      <td>2024-11-25 13:06:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DOGE will take care of that. Everyday consumer...</td>\n",
       "      <td>Cybertruck's Many Recalls Make It Worse Than 9...</td>\n",
       "      <td>2024-11-25 13:06:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>I think we’re likely to be survived only by ou...</td>\n",
       "      <td>Droidspeak: AI models work together faster whe...</td>\n",
       "      <td>2024-11-24 12:15:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>&gt; Security researchers have disrupted a major ...</td>\n",
       "      <td>Dangerous global botnet fueling residential pr...</td>\n",
       "      <td>2024-11-24 03:46:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>I work in security and really don’t like the n...</td>\n",
       "      <td>Dangerous global botnet fueling residential pr...</td>\n",
       "      <td>2024-11-24 03:46:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>ask crowdstrike, most groups follow their nami...</td>\n",
       "      <td>Dangerous global botnet fueling residential pr...</td>\n",
       "      <td>2024-11-24 03:46:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>Maddening.</td>\n",
       "      <td>Dangerous global botnet fueling residential pr...</td>\n",
       "      <td>2024-11-24 03:46:44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>617 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  body  \\\n",
       "0         I wonder which cars are in the remaining 9%?   \n",
       "1    Since launch, Tesla's polarizing electric pick...   \n",
       "2    which 9% of cars make up a list of vehicles WO...   \n",
       "3    It was never purchased for utility, so to the ...   \n",
       "4    DOGE will take care of that. Everyday consumer...   \n",
       "..                                                 ...   \n",
       "612  I think we’re likely to be survived only by ou...   \n",
       "613  > Security researchers have disrupted a major ...   \n",
       "614  I work in security and really don’t like the n...   \n",
       "615  ask crowdstrike, most groups follow their nami...   \n",
       "616                                         Maddening.   \n",
       "\n",
       "                                            post_title            post_time  \n",
       "0    Cybertruck's Many Recalls Make It Worse Than 9...  2024-11-25 13:06:48  \n",
       "1    Cybertruck's Many Recalls Make It Worse Than 9...  2024-11-25 13:06:48  \n",
       "2    Cybertruck's Many Recalls Make It Worse Than 9...  2024-11-25 13:06:48  \n",
       "3    Cybertruck's Many Recalls Make It Worse Than 9...  2024-11-25 13:06:48  \n",
       "4    Cybertruck's Many Recalls Make It Worse Than 9...  2024-11-25 13:06:48  \n",
       "..                                                 ...                  ...  \n",
       "612  Droidspeak: AI models work together faster whe...  2024-11-24 12:15:17  \n",
       "613  Dangerous global botnet fueling residential pr...  2024-11-24 03:46:44  \n",
       "614  Dangerous global botnet fueling residential pr...  2024-11-24 03:46:44  \n",
       "615  Dangerous global botnet fueling residential pr...  2024-11-24 03:46:44  \n",
       "616  Dangerous global botnet fueling residential pr...  2024-11-24 03:46:44  \n",
       "\n",
       "[617 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments.to_csv('red_data_2.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labelling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'red_data_2.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Check the first few rows to understand the structure\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I wonder which cars are in the remaining 9%?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Since launch, Tesla's polarizing electric pick...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>which 9% of cars make up a list of vehicles WO...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It was never purchased for utility, so to the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DOGE will take care of that. Everyday consumer...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  score\n",
       "0       I wonder which cars are in the remaining 9%?      1\n",
       "1  Since launch, Tesla's polarizing electric pick...      0\n",
       "2  which 9% of cars make up a list of vehicles WO...      0\n",
       "3  It was never purchased for utility, so to the ...      1\n",
       "4  DOGE will take care of that. Everyday consumer...      2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to classify sentiment using VADER\n",
    "def classify_sentiment_vader(text):\n",
    "    sentiment_score = analyzer.polarity_scores(str(text))\n",
    "    # Classify as negative, neutral, or positive based on compound score\n",
    "    if sentiment_score['compound'] >= 0.05:\n",
    "        return 2  # Positive\n",
    "    elif sentiment_score['compound'] <= -0.05:\n",
    "        return 0  # Negative\n",
    "    else:\n",
    "        return 1  # Neutral\n",
    "\n",
    "# Apply the VADER sentiment classification to the 'body' column\n",
    "data['score'] = data['body'].apply(classify_sentiment_vader)\n",
    "\n",
    "# Check the results\n",
    "data[['body', 'score']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('red_data_3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAGE 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing Sentimental Analysis on YT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API key here\n",
    "API_KEY = \"AIzaSyCKz6fxf2IvH8z8LIp5mpm76LB9u9fxHUU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Youtube Comments Extraction using YT.v3 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get comments for a given video ID\n",
    "def get_comments(video_id, max_comments=100, order=\"relevance\"):\n",
    "    youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "    \n",
    "    comments = []\n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        videoId=video_id,\n",
    "        maxResults=max_comments,\n",
    "        textFormat=\"plainText\",\n",
    "        order=order\n",
    "    )\n",
    "    \n",
    "    while request and len(comments) < max_comments:\n",
    "        try:\n",
    "            response = request.execute()\n",
    "        \n",
    "            # Loop through each comment in the response\n",
    "            for item in response.get(\"items\", []):\n",
    "                full_comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "                comments.append(full_comment)  # Store the full comment\n",
    "                \n",
    "                # Stop if we have reached the max comments\n",
    "                if len(comments) >= max_comments:\n",
    "                    break\n",
    "        \n",
    "            # Check for more comments (pagination)\n",
    "            request = youtube.commentThreads().list_next(request, response)\n",
    "        except HttpError as e:\n",
    "            if \"commentsDisabled\" in str(e):\n",
    "                print(f\"Comments are disabled for Video ID: {video_id}. Skipping this video.\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"An unexpected error occurred with Video ID: {video_id}. Error: {e}\")\n",
    "                break\n",
    "\n",
    "    return comments[:max_comments]  # Return up to max_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_videos(query, max_videos=10):\n",
    "    youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "    \n",
    "    # Search for videos based on the query\n",
    "    request = youtube.search().list(\n",
    "        q=query,\n",
    "        part=\"snippet\",\n",
    "        order=\"viewCount\",\n",
    "        maxResults=max_videos,\n",
    "        type=\"video\"\n",
    "    )\n",
    "    \n",
    "    response = request.execute()\n",
    "    \n",
    "    video_data = []  # List to hold video data and comments\n",
    "\n",
    "    for item in response.get(\"items\", []):\n",
    "        video_id = item[\"id\"][\"videoId\"]\n",
    "        video_title = item[\"snippet\"][\"title\"]  # Get video title\n",
    "        print(f\"Fetching comments for Video ID: {video_id}\") \n",
    "        \n",
    "        # Get comments and handle errors (skipping videos with disabled comments)\n",
    "        comments = get_comments(video_id)\n",
    "        \n",
    "        if comments:  # If comments were successfully fetched\n",
    "            for comment in comments:\n",
    "                video_data.append({\"video_id\": video_id, \"video_title\": video_title, \"comment\": comment})\n",
    "    \n",
    "    return video_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USER INPUT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching comments for Video ID: 2cukB4_hDCI\n",
      "Fetching comments for Video ID: DxREm3s1scA\n",
      "Fetching comments for Video ID: cpraXaw7dyc\n",
      "Fetching comments for Video ID: 2lLZ9AWhcNo\n",
      "Fetching comments for Video ID: XiQkeWOFwmk\n",
      "Fetching comments for Video ID: fgm5uZaS3-E\n",
      "Fetching comments for Video ID: DB1027Bfpmo\n",
      "Fetching comments for Video ID: Mu-eK72ioDk\n",
      "Fetching comments for Video ID: nAgTgwak7ME\n",
      "Fetching comments for Video ID: 8vsTNFUFJEU\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2cukB4_hDCI</td>\n",
       "      <td>Robots testing the Bulletproof #cybertruck</td>\n",
       "      <td>Robots with guns does not give me a warm and f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2cukB4_hDCI</td>\n",
       "      <td>Robots testing the Bulletproof #cybertruck</td>\n",
       "      <td>\"Yee haw\" killed me💀💀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2cukB4_hDCI</td>\n",
       "      <td>Robots testing the Bulletproof #cybertruck</td>\n",
       "      <td>Dude : \"Now give me back my gun\"\\nRobot : \"giv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2cukB4_hDCI</td>\n",
       "      <td>Robots testing the Bulletproof #cybertruck</td>\n",
       "      <td>Guy: Nice job! I’ll take my gun back.\\nRobot: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2cukB4_hDCI</td>\n",
       "      <td>Robots testing the Bulletproof #cybertruck</td>\n",
       "      <td>*10 seconds later*\\nRobot:\"Now we have to chex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                 video_title  \\\n",
       "0  2cukB4_hDCI  Robots testing the Bulletproof #cybertruck   \n",
       "1  2cukB4_hDCI  Robots testing the Bulletproof #cybertruck   \n",
       "2  2cukB4_hDCI  Robots testing the Bulletproof #cybertruck   \n",
       "3  2cukB4_hDCI  Robots testing the Bulletproof #cybertruck   \n",
       "4  2cukB4_hDCI  Robots testing the Bulletproof #cybertruck   \n",
       "\n",
       "                                             comment  \n",
       "0  Robots with guns does not give me a warm and f...  \n",
       "1                              \"Yee haw\" killed me💀💀  \n",
       "2  Dude : \"Now give me back my gun\"\\nRobot : \"giv...  \n",
       "3  Guy: Nice job! I’ll take my gun back.\\nRobot: ...  \n",
       "4  *10 seconds later*\\nRobot:\"Now we have to chex...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"tesla we robot\"  # Your search query\n",
    "top_videos_comments = get_top_videos(query, max_videos=10)  # Fetch top 10 videos\n",
    "\n",
    "# Convert the results into a DataFrame for better readability\n",
    "df = pd.DataFrame(top_videos_comments)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2cukB4_hDCI</td>\n",
       "      <td>Robots testing the Bulletproof #cybertruck</td>\n",
       "      <td>Robots with guns does not give me a warm and f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2cukB4_hDCI</td>\n",
       "      <td>Robots testing the Bulletproof #cybertruck</td>\n",
       "      <td>\"Yee haw\" killed me💀💀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2cukB4_hDCI</td>\n",
       "      <td>Robots testing the Bulletproof #cybertruck</td>\n",
       "      <td>Dude : \"Now give me back my gun\"\\nRobot : \"giv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2cukB4_hDCI</td>\n",
       "      <td>Robots testing the Bulletproof #cybertruck</td>\n",
       "      <td>Guy: Nice job! I’ll take my gun back.\\nRobot: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2cukB4_hDCI</td>\n",
       "      <td>Robots testing the Bulletproof #cybertruck</td>\n",
       "      <td>*10 seconds later*\\nRobot:\"Now we have to chex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>8vsTNFUFJEU</td>\n",
       "      <td>Tesla Optimus Bot FOLDS the Laundry !</td>\n",
       "      <td>“Hey Jerry, you got that jack rabbit chip?”\\nJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>8vsTNFUFJEU</td>\n",
       "      <td>Tesla Optimus Bot FOLDS the Laundry !</td>\n",
       "      <td>When you see a Terminator and it asks for your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>8vsTNFUFJEU</td>\n",
       "      <td>Tesla Optimus Bot FOLDS the Laundry !</td>\n",
       "      <td>That shirt gone be wrinkled as hell 😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>8vsTNFUFJEU</td>\n",
       "      <td>Tesla Optimus Bot FOLDS the Laundry !</td>\n",
       "      <td>Such smooth movements. No arthritis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>8vsTNFUFJEU</td>\n",
       "      <td>Tesla Optimus Bot FOLDS the Laundry !</td>\n",
       "      <td>This is gonna be the start of the robots right...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        video_id                                 video_title  \\\n",
       "0    2cukB4_hDCI  Robots testing the Bulletproof #cybertruck   \n",
       "1    2cukB4_hDCI  Robots testing the Bulletproof #cybertruck   \n",
       "2    2cukB4_hDCI  Robots testing the Bulletproof #cybertruck   \n",
       "3    2cukB4_hDCI  Robots testing the Bulletproof #cybertruck   \n",
       "4    2cukB4_hDCI  Robots testing the Bulletproof #cybertruck   \n",
       "..           ...                                         ...   \n",
       "995  8vsTNFUFJEU       Tesla Optimus Bot FOLDS the Laundry !   \n",
       "996  8vsTNFUFJEU       Tesla Optimus Bot FOLDS the Laundry !   \n",
       "997  8vsTNFUFJEU       Tesla Optimus Bot FOLDS the Laundry !   \n",
       "998  8vsTNFUFJEU       Tesla Optimus Bot FOLDS the Laundry !   \n",
       "999  8vsTNFUFJEU       Tesla Optimus Bot FOLDS the Laundry !   \n",
       "\n",
       "                                               comment  \n",
       "0    Robots with guns does not give me a warm and f...  \n",
       "1                                \"Yee haw\" killed me💀💀  \n",
       "2    Dude : \"Now give me back my gun\"\\nRobot : \"giv...  \n",
       "3    Guy: Nice job! I’ll take my gun back.\\nRobot: ...  \n",
       "4    *10 seconds later*\\nRobot:\"Now we have to chex...  \n",
       "..                                                 ...  \n",
       "995  “Hey Jerry, you got that jack rabbit chip?”\\nJ...  \n",
       "996  When you see a Terminator and it asks for your...  \n",
       "997              That shirt gone be wrinkled as hell 😂  \n",
       "998                Such smooth movements. No arthritis  \n",
       "999  This is gonna be the start of the robots right...  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('red_data_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_results_yt(query):\n",
    "    top_videos_comments = get_top_videos(query, max_videos=10)  # Fetch top 10 videos\n",
    "\n",
    "    # Convert the results into a DataFrame for better readability\n",
    "    df = pd.DataFrame(top_videos_comments)\n",
    "    df.to_csv('red_data_4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAGE 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Model begins here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model v3.0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('red_data_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to Dataset format compatible with Hugging Face Trainer\n",
    "def preprocess_data(df):\n",
    "    return {'text': df['body'], 'label': df['score']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(df))\n",
    "train_data = df[:train_size]\n",
    "test_data = df[train_size:]\n",
    "\n",
    "# Apply preprocessing and convert the data into the proper format\n",
    "preprocessed_train = df[:train_size].apply(preprocess_data, axis=1).to_list()\n",
    "preprocessed_test = df[train_size:].apply(preprocess_data, axis=1).to_list()\n",
    "# Convert preprocessed data into Dataset objects\n",
    "train_dataset = Dataset.from_list(preprocessed_train)\n",
    "test_dataset = Dataset.from_list(preprocessed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the datasets\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae16140a33a040a49f722316a1ced6b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/392 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b677c55ae825479483ed114c4282b3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/98 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "# Set format to PyTorch tensors for Trainer\n",
    "tokenized_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lab\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',               # output directory\n",
    "    evaluation_strategy=\"epoch\",          # evaluation strategy (evaluate at the end of each epoch)\n",
    "    learning_rate=2e-5,                   # learning rate\n",
    "    per_device_train_batch_size=16,       # batch size for training\n",
    "    per_device_eval_batch_size=16,        # batch size for evaluation\n",
    "    num_train_epochs=3,                   # number of training epochs\n",
    "    weight_decay=0.01,                    # strength of weight decay\n",
    "    logging_dir='./logs',                 # directory for storing logs\n",
    "    logging_steps=10,                     # log every 10 steps\n",
    "    save_strategy=\"epoch\"                 # save model at the end of every epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred: EvalPrediction):\n",
    "    predictions, labels = pred.predictions, pred.label_ids\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1_score': f1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e15ba176cb44328c092ed287337876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lab\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1097, 'grad_norm': 3.8403055667877197, 'learning_rate': 1.7333333333333336e-05, 'epoch': 0.4}\n",
      "{'loss': 1.0669, 'grad_norm': 4.20644998550415, 'learning_rate': 1.4666666666666666e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fcac4ad291b4f38a8e9a16d0e0070cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0226513147354126, 'eval_accuracy': 0.4897959183673469, 'eval_f1_score': 0.43321247863974693, 'eval_runtime': 0.8654, 'eval_samples_per_second': 113.237, 'eval_steps_per_second': 8.088, 'epoch': 1.0}\n",
      "{'loss': 1.0427, 'grad_norm': 3.9822816848754883, 'learning_rate': 1.2e-05, 'epoch': 1.2}\n",
      "{'loss': 0.9613, 'grad_norm': 5.70676851272583, 'learning_rate': 9.333333333333334e-06, 'epoch': 1.6}\n",
      "{'loss': 0.9851, 'grad_norm': 6.892285346984863, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675cae60b681460385d5fdbed2f86158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0184816122055054, 'eval_accuracy': 0.45918367346938777, 'eval_f1_score': 0.366649502343256, 'eval_runtime': 0.839, 'eval_samples_per_second': 116.81, 'eval_steps_per_second': 8.344, 'epoch': 2.0}\n",
      "{'loss': 0.8918, 'grad_norm': 9.054837226867676, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.4}\n",
      "{'loss': 0.8894, 'grad_norm': 8.25114631652832, 'learning_rate': 1.3333333333333334e-06, 'epoch': 2.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3efd3d63cfa4e47abe23d89c89c845d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0140188932418823, 'eval_accuracy': 0.46938775510204084, 'eval_f1_score': 0.38917230433265415, 'eval_runtime': 0.8339, 'eval_samples_per_second': 117.517, 'eval_steps_per_second': 8.394, 'epoch': 3.0}\n",
      "{'train_runtime': 40.7057, 'train_samples_per_second': 28.89, 'train_steps_per_second': 1.842, 'train_loss': 0.9810653940836589, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=75, training_loss=0.9810653940836589, metrics={'train_runtime': 40.7057, 'train_samples_per_second': 28.89, 'train_steps_per_second': 1.842, 'total_flos': 309421379248128.0, 'train_loss': 0.9810653940836589, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_bert_tokenizer\\\\tokenizer_config.json',\n",
       " './fine_tuned_bert_tokenizer\\\\special_tokens_map.json',\n",
       " './fine_tuned_bert_tokenizer\\\\vocab.txt',\n",
       " './fine_tuned_bert_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine_tuned_bert_model')\n",
    "tokenizer.save_pretrained('./fine_tuned_bert_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('./fine_tuned_bert_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('./fine_tuned_bert_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Function to predict sentiment with percentage outputs\n",
    "def predict_sentiment_with_percentages(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits     # Get logits from model output\n",
    "    probs = F.softmax(logits, dim=-1).flatten()     # Convert logits to probabilities using softmax\n",
    "    sentiment_labels = ['negative', 'neutral', 'positive']     # Map the probabilities to their respective sentiment labels\n",
    "    sentiment_percentages = {label: round(prob.item() * 100, 2) for label, prob in zip(sentiment_labels, probs)}\n",
    "    return sentiment_percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'negative': 23.18, 'neutral': 22.91, 'positive': 53.91}\n"
     ]
    }
   ],
   "source": [
    "# Example prediction\n",
    "text = \"Should i buy a tesla as Emmanuel said it is good but it also very costly\"\n",
    "result = predict_sentiment_with_percentages(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process Starts Here...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results():\n",
    "    df = pd.read_csv('red_data_4.csv')\n",
    "\n",
    "    sentiment_results = []\n",
    "\n",
    "    # Iterate over the texts in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['comment']\n",
    "        sentiment_percentages = predict_sentiment_with_percentages(text)\n",
    "        sentiment_results.append({\n",
    "            'comment': text,\n",
    "            'negative': sentiment_percentages['negative'],\n",
    "            'neutral': sentiment_percentages['neutral'],\n",
    "            'positive': sentiment_percentages['positive']\n",
    "        })\n",
    "\n",
    "    sentiment_df = pd.DataFrame(sentiment_results)\n",
    "    return sentiment_df['positive'].mean().__round__(2), sentiment_df['neutral'].mean().__round__(2), sentiment_df['negative'].mean().__round__(2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Robots with guns does not give me a warm and f...</td>\n",
       "      <td>42.29</td>\n",
       "      <td>15.06</td>\n",
       "      <td>42.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Yee haw\" killed me💀💀</td>\n",
       "      <td>16.53</td>\n",
       "      <td>49.97</td>\n",
       "      <td>33.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dude : \"Now give me back my gun\"\\nRobot : \"giv...</td>\n",
       "      <td>27.58</td>\n",
       "      <td>27.65</td>\n",
       "      <td>44.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Guy: Nice job! I’ll take my gun back.\\nRobot: ...</td>\n",
       "      <td>40.93</td>\n",
       "      <td>10.46</td>\n",
       "      <td>48.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>*10 seconds later*\\nRobot:\"Now we have to chex...</td>\n",
       "      <td>36.79</td>\n",
       "      <td>22.58</td>\n",
       "      <td>40.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>“Hey Jerry, you got that jack rabbit chip?”\\nJ...</td>\n",
       "      <td>21.54</td>\n",
       "      <td>26.09</td>\n",
       "      <td>52.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>When you see a Terminator and it asks for your...</td>\n",
       "      <td>32.86</td>\n",
       "      <td>12.27</td>\n",
       "      <td>54.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>That shirt gone be wrinkled as hell 😂</td>\n",
       "      <td>33.81</td>\n",
       "      <td>21.01</td>\n",
       "      <td>45.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Such smooth movements. No arthritis</td>\n",
       "      <td>31.91</td>\n",
       "      <td>33.74</td>\n",
       "      <td>34.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>This is gonna be the start of the robots right...</td>\n",
       "      <td>24.97</td>\n",
       "      <td>31.24</td>\n",
       "      <td>43.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               comment  negative  neutral  \\\n",
       "0    Robots with guns does not give me a warm and f...     42.29    15.06   \n",
       "1                                \"Yee haw\" killed me💀💀     16.53    49.97   \n",
       "2    Dude : \"Now give me back my gun\"\\nRobot : \"giv...     27.58    27.65   \n",
       "3    Guy: Nice job! I’ll take my gun back.\\nRobot: ...     40.93    10.46   \n",
       "4    *10 seconds later*\\nRobot:\"Now we have to chex...     36.79    22.58   \n",
       "..                                                 ...       ...      ...   \n",
       "995  “Hey Jerry, you got that jack rabbit chip?”\\nJ...     21.54    26.09   \n",
       "996  When you see a Terminator and it asks for your...     32.86    12.27   \n",
       "997              That shirt gone be wrinkled as hell 😂     33.81    21.01   \n",
       "998                Such smooth movements. No arthritis     31.91    33.74   \n",
       "999  This is gonna be the start of the robots right...     24.97    31.24   \n",
       "\n",
       "     positive  \n",
       "0       42.64  \n",
       "1       33.49  \n",
       "2       44.77  \n",
       "3       48.61  \n",
       "4       40.63  \n",
       "..        ...  \n",
       "995     52.38  \n",
       "996     54.86  \n",
       "997     45.19  \n",
       "998     34.35  \n",
       "999     43.79  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive:  43.271089999999994\n",
      "Neutral:  27.8117\n",
      "Negative:  28.917300000000004\n"
     ]
    }
   ],
   "source": [
    "print(\"Positive: \",sentiment_df['positive'].mean())\n",
    "print(\"Neutral: \",sentiment_df['neutral'].mean())\n",
    "print(\"Negative: \",sentiment_df['negative'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAGE 4 (DO NOT RUN THIS!) - MODEL NOT STABLE⚠️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT GEN Model begins here...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# Assign a padding token if not already present\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as the pad_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input text and set up labels\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()  # Set the labels as input_ids\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Convert dataset to PyTorch tensors\n",
    "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = tokenized_datasets[\"train\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tokenizer.pad(batch, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=1,  # Disable multiprocessing to debug\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "scaler = GradScaler()  # Initialize the scaler for mixed precision\n",
    "\n",
    "# Set gradient accumulation steps (adjust to simulate larger batches)\n",
    "accumulation_steps = 4  # Simulates larger batch size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(train_dataloader, leave=True)\n",
    "\n",
    "    optimizer.zero_grad()  # Reset the gradients before starting\n",
    "\n",
    "    for step, batch in enumerate(loop):\n",
    "        inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "\n",
    "        with torch.cuda.amp.autocast():  # Enable mixed precision\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss / accumulation_steps  # Scale loss for accumulation\n",
    "\n",
    "        scaler.scale(loss).backward()  # Backpropagate loss\n",
    "\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)  # Update weights\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lab\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lab\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are you? Tell me about Tesla Motors!\n",
      "I'm a big fan of the company and I've been working on it for years. It's one of my favorite cars ever made, but there is something special that makes this car so unique to us: The way we build our vehicles in such an innovative manner allows them not only to be built with high quality materials (like aluminum), they can also have their own custom parts available from suppliers like BMW or Mercedes-Benz as well – all without having to worry too much over what will happen when these components go into production…and then get shipped out by truck every year.\"\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set pad token to eos token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Move model to the correct device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Prepare input text and move to device\n",
    "input_text = \"How are you? Tell me about Tesla Motors!\"\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Create attention mask to differentiate between padding and actual data\n",
    "attention_mask = torch.ones(inputs.shape, device=device)\n",
    "\n",
    "# Generate text with repetition penalty\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=150,  # Increased length to avoid truncation\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        temperature=0.8,  # Slightly adjusted temperature\n",
    "        top_k=50,         # Limit next token choices to top-k\n",
    "        top_p=0.9,        # Use nucleus sampling\n",
    "        repetition_penalty=1.2  # Penalty to discourage repetition\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAGE 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"AIzaSyDYCwFViLa0Ry51BxOdLYZSU4JdkMF3kSY\")\n",
    "\n",
    "model2 = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_tex(user_input):\n",
    "\n",
    "    response = model2.generate_content(f\"write a mail to {user_input} for partnership with my company in the future\")\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAGE 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public URL: NgrokTunnel: \"https://312c-115-247-147-18.ngrok-free.app\" -> \"http://localhost:5000\"    \n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [27/Nov/2024 18:35:28] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Nov/2024 18:35:29] \"GET /favicon.ico HTTP/1.1\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching comments for Video ID: pBHwJkrz3x4\n",
      "Fetching comments for Video ID: 6R6F371Hj3k\n",
      "Comments are disabled for Video ID: 6R6F371Hj3k. Skipping this video.\n",
      "Fetching comments for Video ID: r1Rrt8iaOUc\n",
      "Fetching comments for Video ID: igW_YJ7r1Zc\n",
      "Fetching comments for Video ID: 43nSDUdse60\n",
      "Fetching comments for Video ID: BDx_YTf9x1g\n",
      "Fetching comments for Video ID: l2q_-xN2N54\n",
      "Fetching comments for Video ID: JC9VVO0aUQw\n",
      "Fetching comments for Video ID: EL1lwZP-RqM\n",
      "Fetching comments for Video ID: 9vwHuCC6nP8\n",
      "An unexpected error occurred with Video ID: 9vwHuCC6nP8. Error: <HttpError 400 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=9vwHuCC6nP8&maxResults=100&textFormat=plainText&order=relevance&key=AIzaSyCKz6fxf2IvH8z8LIp5mpm76LB9u9fxHUU&alt=json returned \"The API server failed to successfully process the request. While this can be a transient error, it usually indicates that the request's input is invalid. Check the structure of the <code>commentThread</code> resource in the request body to ensure that it is valid.\". Details: \"[{'message': \"The API server failed to successfully process the request. While this can be a transient error, it usually indicates that the request's input is invalid. Check the structure of the <code>commentThread</code> resource in the request body to ensure that it is valid.\", 'domain': 'youtube.commentThread', 'reason': 'processingFailure', 'location': 'body', 'locationType': 'other'}]\">\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Nov/2024 18:36:34] \"POST / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search: tesla\n",
      "Sentiment scores: Positive: 38.99, Neutral: 27.78, Negative: 33.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Nov/2024 18:36:46] \"POST / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search: tesla\n",
      "Sentiment scores: Positive: 38.99, Neutral: 27.78, Negative: 33.23\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "app.secret_key = \"mypopeshighonweed\"\n",
    "ngrok.set_auth_token(\"2pQvQx6Ujbi5CAsfXIJeo4AVoOl_6fgRD49CmZMN2zFd9xrtw\")\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def home():\n",
    "    user_input = \"\"\n",
    "    sentiment = session.get('sentiment', None)\n",
    "    generated_text = None\n",
    "\n",
    "    if request.method == 'POST':\n",
    "        # Get the data from the input box\n",
    "        user_input = request.form['user_input'].strip()\n",
    "\n",
    "        if not user_input:  # Validate input\n",
    "            generated_text = \"Please enter a valid input before submitting.\"\n",
    "        elif 'search' in request.form:\n",
    "            search_results = get_search_results_yt(user_input)\n",
    "            p, neu, neg = generate_results()\n",
    "            sentiment = f\"Positive: {p}, Neutral: {neu}, Negative: {neg}\"\n",
    "            session['sentiment'] = sentiment\n",
    "        elif 'generate' in request.form:\n",
    "            generated_text = gen_tex(user_input)\n",
    "\n",
    "        # Debugging print statements (optional)\n",
    "        print(f\"Search: {user_input}\")\n",
    "        print(f\"Sentiment scores: {sentiment}\")\n",
    "\n",
    "    return render_template_string(\"\"\"\n",
    "        <form method=\"POST\">\n",
    "            <input type=\"text\" name=\"user_input\" placeholder=\"Enter something\" value=\"{{ user_input }}\">\n",
    "            <button type=\"submit\" name=\"search\">Search</button>\n",
    "            <button type=\"submit\" name=\"generate\">Generate</button>\n",
    "        </form>\n",
    "        {% if user_input %}\n",
    "            <h3>Searching: {{ user_input }}</h3>\n",
    "            <p>Results: {{ sentiment }}</p>\n",
    "        {% endif %}\n",
    "        {% if user_input and generated_text %}\n",
    "            <h3>Generated Text:</h3>\n",
    "            <p>{{ generated_text }}</p>\n",
    "        {% endif %}\n",
    "    \"\"\", user_input=user_input, sentiment=sentiment, generated_text=generated_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This will keep the Flask app running until you manually stop it (using Ctrl+C)\n",
    "    # app.run(debug=True, use_reloader=False, port = 5001)\n",
    "    # Start the ngrok tunnel\n",
    "    public_url = ngrok.connect(5000)\n",
    "    print(f\"Public URL: {public_url}\")\n",
    "    # Run the Flask app\n",
    "    app.run(port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
